# -*- coding: utf-8 -*-
"""predictiv_analysis_diamons.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i0QvCxFV91UOpl8eK0URBEkTeW3lA9t5
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

url = "https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv"
diamons = pd.read_csv(url)
diamons

"""# Exploratory Data Analysis - Deskripsi Variabel

- Harga dalam dolar Amerika Serikat ($) adalah fitur target.
- carat: merepresentasikan bobot (weight) dari diamonds (0.2-5.01), digunakan sebagai ukuran dari batu permata dan perhiasan.
- cut: merepresentasikan kualitas pemotongan diamonds (Fair, Good, Very Good, Premium, and Ideal).
- color: merepresentasikan warna, dari J (paling buruk) ke D (yang terbaik).
- clarity: merepresentasikan seberapa jernih diamonds (I1 (paling buruk), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (terbaik))
- x: merepresentasikan panjang diamonds dalam mm (0-10.74).
- y: merepresentasikan lebar diamonds dalam mm (0-58.9).
- z: merepresentasikan kedalaman diamonds dalam mm (0-31.8).
- depth: merepresentasikan z/mean(x, y) = 2 * z/(x + y) (43-79).
- table: merepresentasikan lebar bagian atas berlian relatif terhadap titik terlebar 43-95).
"""

diamons.info()

"""* terdapat 3 kolom dengan tipe object (non numeric)
* terdapat 6 kolom dengan tipe float (numeric)
* terdapat 1 kolom numerik dengan tipe data int
"""

diamons.describe()

"""# Missing Value"""

x = (diamons.x == 0).sum()
y = (diamons.y == 0).sum()
z = (diamons.z == 0).sum()

print('Nilai 0 di kolom x ada : ', x)
print('Nilai 0 di kolom y ada : ', y)
print('Nilai 0 di kolom z ada : ', z)

diamons.loc[(diamons['z'] == 0)]

diamons = diamons.loc[(diamons[['x','y','z']]!=0).all(axis=1)]

diamons.shape

diamons.describe()

"""# Menangani Outliers

* teknik menangani ouliers
1. Hypothesis Testing
2. Z-score method
3. IQR Method

* IQR
"""

Q1 = diamons.quantile(0.25)
Q3 = diamons.quantile(0.75)
IQR = Q3 - Q1
diamons = diamons[~((diamons < (Q1 - 1.5 * IQR))|(diamons > (Q3 + 1.5 * IQR))).any(axis=1)]

# cek ukuran dataset setelah kita drop duplicaters
diamons.shape

"""# EDA (Explorati Data Analys)

## melakukan analysis
Univariate EDA
"""

numerical_features = ['price','carat','depth','table','x','y','z']
categorical_features = ['cut','color','clarity']

"""feature cut"""

feature = categorical_features[0]
count = diamons[feature].value_counts()
percent = 100*diamons[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sample': count, 'persentase' : percent.round(1)})
print(df)

count.plot(kind='bar', title=feature)
plt.show()

"""dari hasil visualisasi diatas terdapat 5 kategori pada fitur cut, yaitu ideal, premium, very good, good, fair. dari data presentase dapat kita simpulkan bahwa lebih dari 60% sampel merupakan diamonds tipe grade tinggi yaitu ideal dan premium

feature color
"""

features = diamons.columns[2]
count = diamons[features].value_counts()
percent = 100*diamons[features].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sample':count, 'percentase':percent.round(1)})
print(df)

count.plot(kind='bar', title=features)
plt.show()

"""berdasarkan deskripsi variabel
urutan warna dari yang paling buruk ke yang paling bagus adalah J,I,H,G,F,E dan D. dari grafik di atas dapat kita simpulkan bahwa sebagian besar grade berada pada grade menengah, yaitu grade G, F, H
"""

feature = diamons.columns[3]
count = diamons[feature].value_counts()
percent = 100*diamons[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sample':count, 'percent':percent.round(1)})
print(df)

count.plot(kind='bar', title=feature)
plt.show()

"""berdasarkan dari deskripsi variabel
fiture clarity terdiri dari 8 kategori dari yang paling buruk ke yang paling baik
yaitu: I1, SI2, SI1, VS2,VS1,VVS2,VVS1,IF

'IF' - Internally Flawless

'VVS2' - Very Very Slight Inclusions

'VVS1' - Very Very Slight Inclusions

'VS1' - Very Slight Inclusions

'VS2' - Very Slight Inclusions

'SI2' - Slight Inclusions

'SI1' - Slight Inclusions

'I1' - Imperfect

### NUMERIC FEATURES
"""

diamons.hist(bins=50, figsize=(20, 15))
plt.show()

"""kita amati histogram 'price' yang akan menjadi target kita
kita bisa memperoleh beberapa informasi, diantaranya :
* peningkatan diamons sebanding dengan penurunan jumlah sampel. hal ini dapat kita lihat jelas dari histogram 'price' yang grafiknya mengalami penurunan seiring dengan semakin banyaknya jumlah sampel (sumbu x)

* rendang harga diamons cukup tinggi yaitu dari skala ratusan dolar amerika hingga sekitar $11800

* setengah harga berilian dibawah $2500

* distribusi harga miring ke kanan (right-skewed) hal ini akan berimplikasi pada model

# EDA - Multivariate Analys

### Categorical Features
"""

cat_feature = diamons.select_dtypes(include='object').columns.to_list()

for col in cat_feature:
  sns.catplot(x=col, y='price', kind='bar', dodge=False, height=4, aspect=3, data=diamons, palette='Set3')
  plt.title('Rata-rata "price" relatif terhadap - {}'.format(col))

"""dengan mengamati rata-rata harga relatif terhadap fitur kategori di atas, kita memperoleh insight sebagai berikut:

* pada fitur 'cut', rata-rata harga cenderung mirip. rentangnya berada antara 3500 hingga 4500. grade tertinggi yaitu grade ideal memiliki harga rata-rata terendah diantara grade lainnya. sehingga, fitur cut memiliki pengaruh atau dampak yang kecil terhadap rata-rata harga.

* pada fitur 'color' semakin rendah grade warna, harga diamonds justru semakin tinggi. dari sini dapat disimpulkan bawa warna memiliki pengaruh yang rendah terhadap harga.

* pada fitur 'clarity', secara umum, diamond dengan grade lebih rendah memiliki harga yang lebih tinggi. hal ini berarti bahwa fitur 'clarity' memiliki pengaruh yang rendah terhadap harga.

Kesimpulan akhir, fitur kategori memiliki pengaruh yang rendah terhadap harga

### Numerical feature
mengamati hubungan antara firut numerik menggunakan fungsi pairplot()
"""

# mengamati hubungan antara fituur numerik dengan fungsi pairplot()
sns.pairplot(diamons, diag_kind= 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = diamons.corr().round(2)

# untuk menge-print nilai dalam kotak, menggunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidth=0.5)
plt.title('Correlation Matrix untuk Fitur Numerik', size=20)
plt.show()

diamons.drop(['depth'], inplace=True, axis=1)
diamons.head()

"""# Data Preparation"""

# melakukan one-hot-encoding untuk fitur yang bertipe kategori

from sklearn.preprocessing import OneHotEncoder

diamons = pd.concat([diamons, pd.get_dummies(diamons['cut'], prefix='cut')], axis=1)
diamons = pd.concat([diamons, pd.get_dummies(diamons['color'], prefix='color')], axis=1)
diamons = pd.concat([diamons, pd.get_dummies(diamons['clarity'], prefix='clarity')], axis=1)

diamons.drop(['cut','color','clarity'], axis=1, inplace=True)
diamons.head()

"""### reduksi dimensi dengan PCA
#### Principal Component Analisis
PAC adalah teknik untuk meredudansi dimensi, mengekstraksi fitur dan mentransformasi data dari "n-dimensional space" ke dalam sistem berkoordinat baru dengan dimensi m
"""

sns.pairplot(diamons[['x','y','z']], plot_kws={'s':3})

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(diamons[['x','y','z']])
princ_comp = pca.transform(diamons[['x','y','z']])

princ_comp

pca.explained_variance_ratio_.round(3)

"""hasil dari output di atas, 99.8% informasi pada ketiga fitur 'x','y','z' terdapat pada pc pertama. sedangkan sisanya, sebesar 0.2% dan 0.1% terdapat pada pc kedua dan ketiga."""

from sklearn.decomposition import PCA

pca = PCA(n_components=1, random_state=123)
pca.fit(diamons[['x','y','z']])
diamons['dimension'] = pca.transform(diamons.loc[:, ('x','y','z')]).flatten()
diamons.drop(['x','y','z'], inplace=True, axis=1)

diamons.head()

"""### train test split

jika data berjumlah kecil maka pembagian 80:20 sangat ideal


jika data berjumlah besar seperti 5juta maka gunakan 90:10 untuk data uji 10 dan data latih 90
"""

from sklearn.model_selection import train_test_split

x = diamons.drop(['price'], axis=1)
y = diamons.price

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=123, test_size=0.1)

print(f'Total # of sample in whole dataset: {len(x)}')
print(f'Total # of sample in train dataset: {len(x_train)}')
print(f'Total # of sample in test dataset: {len(x_test)}')

from sklearn.preprocessing import StandardScaler

numerical_features = ['carat', 'table', 'dimension']
scaler = StandardScaler()
scaler.fit(x_train[numerical_features])
x_train[numerical_features] = scaler.transform(x_train.loc[:, numerical_features])
x_train[numerical_features].head()

x_train[numerical_features].describe().round(4)

"""### Model Development"""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

# K-Nearst Neighbor (classification / regression)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error


knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(x_train, y_train)

models.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn.predict(x_train), y_true=y_train)

models

# RandomForestRegressor (classifikation / regression)
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
rf.fit(x_train, y_train)

models.loc['train_mse', 'RandomForest'] = mean_squared_error(y_pred = rf.predict(x_train), y_true=y_train)

models

# boosting (regression)
## Adaptive boosting
## Gradient boosting

# adaptive boosting (-)
from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(x_train, y_train)
models.loc['train_mse', 'Boosting'] = mean_squared_error(y_pred = boosting.predict(x_train), y_true = y_train )

models

"""### Evaluation Model"""

# scaling befor evaluation
# lakukan scaling pada feature numeric pada x_test sehingga miliki rata-rata=0 dan varians=1

x_test.loc[:, numerical_features] = scaler.transform(x_test[numerical_features])

# melakukan evaluasi
# buat variable mse yang isinya adalh dataframe nilai mse data train dan data test pada masing-masing algoritma

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# buat dictionary untuk setiapp algoritma yang digunakan
model_dict = {'KNN':knn, 'RF':rf, 'Boosting':boosting}

# hitung MSE masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
  mse.loc[name, 'train'] = mean_squared_error(y_true = y_train, y_pred = model.predict(x_train))/1e3
  mse.loc[name, 'test'] = mean_squared_error(y_true = y_test, y_pred = model.predict(x_test))/1e3

mse

fix, ax = plt.subplots()

mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

# ujicoba harga diamond dengan algoritma yang memiliki error kecil

prediksi = x_test.iloc[:2].copy()
pred_dict = {'y_true':y_test[:2]}

for name, model in model_dict.items():
  pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""random forest memberikan hasil yang paling mendekati nilai sungguh"""

prediksi

